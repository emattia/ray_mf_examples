{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: METAFLOW_PROFILE=local\n"
     ]
    }
   ],
   "source": [
    "%env METAFLOW_PROFILE=local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the server\n",
    "\n",
    "```\n",
    "serve run server:batch_preds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from base import TabularBatchPrediction as TBP\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_world_output = requests.post(\"http://localhost:8000\").json()\n",
    "assert hello_world_output == \"Hello World!\", \"Are you sure you're running the server?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data() -> pd.DataFrame:\n",
    "    tbp = TBP()\n",
    "    _, valid_dataset, test_dataset = tbp.load_dataset()\n",
    "    df = test_dataset.to_pandas()\n",
    "    df['id'] = df.index\n",
    "    return df, valid_dataset.to_pandas()['target'].values\n",
    "\n",
    "def prepare_post_body(batch: pd.DataFrame) -> dict:\n",
    "    id_to_batch = {}\n",
    "    for record in batch:\n",
    "        _id = record.pop('id')\n",
    "        id_to_batch[_id] = record\n",
    "    return id_to_batch\n",
    "\n",
    "def query_predict_endpoint(batch_size = 5, df = None):\n",
    "    if df is None:\n",
    "        df = get_data()\n",
    "    batch = df.sample(batch_size).to_dict('records')\n",
    "    id_to_batch_features = prepare_post_body(batch)\n",
    "    output = requests.post(\"http://localhost:8000/predict/\", data=json.dumps(id_to_batch_features)).json()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No dataset name or loader provided. Using default breast_cancer.csv dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 23:59:08,182\tINFO read_api.py:374 -- To satisfy the requested parallelism of 20, each read task output will be split into 20 smaller blocks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04720929eb84abe9cb9bff0aaaae958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Read progress 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 23:59:08,768\tWARNING plan.py:567 -- Warning: The Ray cluster currently does not have any available CPUs. The Dataset job will hang unless more CPUs are freed up. A common reason is that cluster resources are used by Actors or Tune trials; see the following link for more details: https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357bcfcda3ba4dfabe5a112866d4b762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Read progress 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 23:59:08,782\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2023-08-20 23:59:08,783\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-20 23:59:08,783\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102ad5d99220460c8c0eb278e7a19b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df, true_targets = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"Prediction proba for sample input {} is {}. True target is {}.\"\n",
    "output = query_predict_endpoint(batch_size = 5, df = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction proba for sample input 127 is 0.8797837495803833. True target is 1.\n",
      "Prediction proba for sample input 169 is 0.11638931185007095. True target is 0.\n",
      "Prediction proba for sample input 153 is 0.8228343725204468. True target is 1.\n",
      "Prediction proba for sample input 88 is 0.561342179775238. True target is 1.\n",
      "Prediction proba for sample input 162 is 0.8228343725204468. True target is 1.\n"
     ]
    }
   ],
   "source": [
    "for id, proba in output.items():\n",
    "    print(msg.format(id, proba, true_targets[int(id)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is running on the `/predict` endpoint on the server?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 00:01:37,634\tINFO streaming_executor.py:92 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(StandardScaler._transform_pandas)->MapBatches(ScoringWrapper)]\n",
      "2023-08-21 00:01:37,635\tINFO streaming_executor.py:93 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-08-21 00:01:37,635\tINFO streaming_executor.py:95 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-08-21 00:01:37,656\tINFO actor_pool_map_operator.py:117 -- MapBatches(StandardScaler._transform_pandas)->MapBatches(ScoringWrapper): Waiting for 1 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705b99b07ef0417c98bfcf58606e4ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 00:01:39,637\tWARNING actor_pool_map_operator.py:267 -- To ensure full parallelization across an actor pool of size 1, the specified batch size should be at most 2. Your configured batch size for this operator was 4096.\n"
     ]
    }
   ],
   "source": [
    "from ray.train.xgboost import XGBoostPredictor\n",
    "import ray\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from metaflow import Run, Flow\n",
    "\n",
    "def select_from_checkpoint_registry(flow_name = \"Train\"):\n",
    "    flow = Flow(flow_name)\n",
    "    run = flow.latest_successful_run\n",
    "    result = run.data.result\n",
    "    return result.checkpoint\n",
    "\n",
    "# duplicated in `query_predict_endpoint` above\n",
    "batch_size = 2\n",
    "if df is None:\n",
    "    df = get_data()\n",
    "batch = df.sample(batch_size).to_dict('records')\n",
    "id_to_batch_features = prepare_post_body(batch)\n",
    "\n",
    "features = ray.data.from_items(list(id_to_batch_features.values()))\n",
    "checkpoint = select_from_checkpoint_registry()\n",
    "predictor = BatchPredictor.from_checkpoint(checkpoint, XGBoostPredictor)\n",
    "preds = predictor.predict(features).to_pandas()['predictions'].values\n",
    "preds_payload = dict(zip(id_to_batch_features.keys(), preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: 0.87978375, 139: 0.6463293}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_payload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
